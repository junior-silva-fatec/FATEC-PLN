{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO6grmwQkLSkGHB4L2ek/nV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tHVX9b8UnRvn"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Aula 04 - Extração de Características (Features) em Texto\n","\n","\n","*   Parte 1 - Corpus\n","*   Parte 2 - Extração de Características em Texto\n"],"metadata":{"id":"jSo02imloYlx"}},{"cell_type":"markdown","source":["## Parte 1 - Trabalhando com Corpora"],"metadata":{"id":"YUXrkED2rNbP"}},{"cell_type":"markdown","source":[],"metadata":{"id":"A-gRrsQup8xp"}},{"cell_type":"markdown","source":["## Parte 2 - Extração de Características\n","\n"],"metadata":{"id":"MeFrAMBeqMqH"}},{"cell_type":"markdown","source":["### Exemplo 1 - Implementando BOW"],"metadata":{"id":"xBemozOjqoi_"}},{"cell_type":"code","source":["# importando a ferramenta que irá criar a representação numérica\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Criando um corpus de documentos que será usado para criação do vocabulário\n","documentos = [\n","    \"gato e cachorro\",\n","    \"gato brinca com cachorro\",\n","    \"gato e lindo\"\n","]\n","\n","# Criando um objeto para ser utilizado: transformar os documentos em vetores\n","vectorizer = CountVectorizer()\n","\n","# Criando a matriz de contagem\n","X = vectorizer.fit_transform(documentos)\n","# fit >>> cria um vocabulário das palavras\n","# transform >>> conta a frequência de cada palavra no corpus\n","\n","# Imprimindo a Matriz e o Vocabulário gerado\n","print(\"Vocabulário: \", vectorizer.vocabulary_)\n","# Cada palavra do vocabulário que foi indexada aparece com seu índice\n","\n","print(\"Matriz BOW:\")\n","print(X.toarray())\n","# Mostra a frequência de cada palavra dentro da matriz\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SyGYGtcdu_5V","executionInfo":{"status":"ok","timestamp":1744071691505,"user_tz":180,"elapsed":13,"user":{"displayName":"Junior Silva","userId":"13201648918475997455"}},"outputId":"ddd695e8-aa1b-4022-8d16-4e8fe7b6d3d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulário:  {'gato': 3, 'cachorro': 1, 'brinca': 0, 'com': 2, 'lindo': 4}\n","Matriz BOW:\n","[[0 1 0 1 0]\n"," [1 1 1 1 0]\n"," [0 0 0 1 1]]\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"CCT0pvj5qYIf"}},{"cell_type":"markdown","source":["### Exemplo 2 - Implementando BOW junto com TF-IDF"],"metadata":{"id":"Var4hAiJqciQ"}},{"cell_type":"code","source":["# Importando as bibliotecas\n","from sklearn.feature_extraction.text import CountVectorizer\n","# Classe que transforma os documentos em vetores e realiza a contagem de frequência\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","# Classe que transforma os documentos em vetores e uma contagem de frequência ponderada\n","\n","# Definindo o corpus\n","documentos = [\n","    \"O cachorro gosta de passear no parque.\",\n","    \"O gato dorme no sofá o dia todo.\",\n","    \"Cachorros e gatos podem ser bons amigos\"\n","]\n","\n","# Criando o modelo BoW\n","vectorizer_bow = CountVectorizer()\n","# Instanciamento da classe em objeto para ser usado\n","X_bow = vectorizer_bow.fit_transform(documentos)\n","# X_bow irá realizar a transformação do vocabulário\n","# fit_transform => transforma cada vetor em um documento com a contagem de frequência\n","\n","# Imprimindo o Vocabulário e a Matriz\n","print(\"Vocabulário BoW:\", vectorizer_bow.vocabulary_)\n","print(\"Matriz BoW:\")\n","print(X_bow.toarray())\n","\n","# Criamos um modelo TF-IDF\n","vectorizer_tfidf = TfidfVectorizer()\n","# Instanciamos a classe e transformamos os dados em objeto\n","X_tfidf = vectorizer_tfidf.fit_transform(documentos)\n","# fit irá realizar a transformação do vocabulário\n","# fit_transform => transforma cada vetor em um documento com a contagem de frequência ponderada\n","\n","# Imprimindo o Vocabulário e a Matriz TF-IDF\n","print(\"Vocabulário TF-IDF:\", vectorizer_tfidf.vocabulary_)\n","print(\"Matriz TF-IDF:\")\n","print(X_tfidf.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MOnin74QzeIK","executionInfo":{"status":"ok","timestamp":1744139803208,"user_tz":180,"elapsed":2532,"user":{"displayName":"Junior Silva","userId":"13201648918475997455"}},"outputId":"20d39b9c-7ef4-47e6-cb0f-af2279fd0728"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulário BoW: {'cachorro': 2, 'gosta': 9, 'de': 4, 'passear': 12, 'no': 10, 'parque': 11, 'gato': 7, 'dorme': 6, 'sofá': 15, 'dia': 5, 'todo': 16, 'cachorros': 3, 'gatos': 8, 'podem': 13, 'ser': 14, 'bons': 1, 'amigos': 0}\n","Matriz BoW:\n","[[0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 0]\n"," [0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1]\n"," [1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0]]\n","Vocabulário TF-IDF: {'cachorro': 2, 'gosta': 9, 'de': 4, 'passear': 12, 'no': 10, 'parque': 11, 'gato': 7, 'dorme': 6, 'sofá': 15, 'dia': 5, 'todo': 16, 'cachorros': 3, 'gatos': 8, 'podem': 13, 'ser': 14, 'bons': 1, 'amigos': 0}\n","Matriz TF-IDF:\n","[[0.         0.         0.42339448 0.         0.42339448 0.\n","  0.         0.         0.         0.42339448 0.32200242 0.42339448\n","  0.42339448 0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.42339448\n","  0.42339448 0.42339448 0.         0.         0.32200242 0.\n","  0.         0.         0.         0.42339448 0.42339448]\n"," [0.40824829 0.40824829 0.         0.40824829 0.         0.\n","  0.         0.         0.40824829 0.         0.         0.\n","  0.         0.40824829 0.40824829 0.         0.        ]]\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"bpfvAYP_qjMX"}},{"cell_type":"markdown","source":["### Exemplo 3 - Realizando o Pré-processamento e a Extração de características do texto.  \n","Implementar as etapas de pré-processamento e representação de texto:\n","\n","\n","*   Limpeza dos dados: removendo caracteres indesejados e normalizando o texto.\n","*   Tokenização, stopwords, lematização/stemming.\n","*   Remoção de stopwords - eliminando palavras comuns que não carregam significado semântico importante.\n","*   Lematização - reduzindo as palavras à sua forma base.\n","*   Representação de Texto - convertendo o texto processado em uma matriz numérica usando o modelo Bag of Words"],"metadata":{"id":"sOF3L6M4qy_v"}},{"cell_type":"code","source":[],"metadata":{"id":"eFW07iFVu6s1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exemplo 4 - WordEmbedding utilizando Word2Vex"],"metadata":{"id":"IpnLHgKM7E5n"}}]}
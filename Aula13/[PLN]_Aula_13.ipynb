{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Aula 13** - Redes Neurais em PLN\n",
        "## Objetivos: ##\n",
        "- Implementar uma Rede Neural Recorrente (RNN) simples em Python para prever a próxima palavra em uma sequência de texto, utilizando a biblioteca TensorFlow/Keras.\n",
        "- Implementar uma Rede Long Short-Term Memory (LSTM) em Python para classificar o sentimento de frases como \"positivo\" ou \"negativo\", utilizando a biblioteca TensorFlow/Keras.\n"
      ],
      "metadata": {
        "id": "NkQlPoJw0Roy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 1: Configuração do Ambiente no Google Colab ###\n"
      ],
      "metadata": {
        "id": "7H1vLvSHpTRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necessárias\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "print(\"Bibliotecas importadas com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fv64b4SpzUs",
        "outputId": "ab3541f5-9a0f-48ee-b0ab-5d3b47768227"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 2: Preparação do Conjunto de Dados ###"
      ],
      "metadata": {
        "id": "IVD9UAYto_mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conjunto de dados de treinamento (pequeno e simplificado)\n",
        "textos_treinamento = [\n",
        "\"eu gosto de programar em python\",\n",
        "\"python é uma linguagem poderosa\",\n",
        "\"programar é divertido com python\",\n",
        "\"aprenda python e seja feliz\",\n",
        "\"gosto de aprender coisas novas\"\n",
        "]\n",
        "\n",
        "print(f\"Textos de treinamento: {textos_treinamento}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRWN6HeNlZQW",
        "outputId": "d193f06d-e6cb-4256-d597-7eeb7a7a6820"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textos de treinamento: ['eu gosto de programar em python', 'python é uma linguagem poderosa', 'programar é divertido com python', 'aprenda python e seja feliz', 'gosto de aprender coisas novas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar o Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# Construir o vocabulário a partir dos textos\n",
        "tokenizer. fit_on_texts(textos_treinamento)\n",
        "# Converter textos em sequências de números\n",
        "sequencias = tokenizer.texts_to_sequences(textos_treinamento)\n",
        "# Imprimir o vocabulário e as sequências geradas print(f\"\\nVocabulário (palavra: índice): (tokenizer.word_index}*)\n",
        "print (f\"Sequências numéricas dos textos: {sequencias}\")\n",
        "# Calcular o tamanho do vocabulário (+1 para incluir o 0 de padding)\n",
        "total_palavras = len(tokenizer.word_index) + 1\n",
        "print(f\"Tamanho total do vocabulário: {total_palavras}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiueZHXzljBQ",
        "outputId": "e6351e09-5551-4325-d20a-a25a933872b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequências numéricas dos textos: [[6, 2, 3, 4, 7, 1], [1, 5, 8, 9, 10], [4, 5, 11, 12, 1], [13, 1, 14, 15, 16], [2, 3, 17, 18, 19]]\n",
            "Tamanho total do vocabulário: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar Entradas (X) e Saidas (y) para a previsão da proxima palavra\n",
        "# a entrada (X) será uma sequência de palavras, e a saída (y) será a palavra seguinte.\n",
        "# Determinar o comprimento máximo das sequências para padding\n",
        "max_comprimento = max(len(seq) for seq in sequencias)\n",
        "print(f\"\\nComprimento máximo das sequências antes do padding: {max_comprimento}\")\n",
        "\n",
        "# Criar pares de entrada (sequência parcial) e saída (próxima palavra)\n",
        "# Ex: \"eu gosto de programar\" -> \"em\"\n",
        "# \"gosto de programar em\" -> \"python\"\n",
        "entradas_X = []\n",
        "saidas_y = []\n",
        "\n",
        "for seq in sequencias:\n",
        "  for i in range(1, len(seq)) :\n",
        "    entradas_X.append(seq[:i]) # A sequência até a palavra atual\n",
        "    saidas_y.append(seq[i]) # A proxima palavra\n",
        "\n",
        "print(f\"Exemplo de entradas_X (parcial): {entradas_X[0:5]}\")\n",
        "print(f\"Exemplo de saidas_y (parcial): {saidas_y[0:5]}\")\n",
        "\n",
        "# Padronizar o comprimento das sequências de entrada\n",
        "# Todas as sequências de entrada precisam ter o mesmo comprimento para a RNN\n",
        "entradas_X_padded = pad_sequences (entradas_X, maxlen=max_comprimento -1, padding='pre')\n",
        "\n",
        "# O maxlen é \"max_comprimento - 1' porque a saída 'y' é a última palavra, então x sempre terá 1 palavra a menos.\n",
        "# Converter as saídas para o formato one-hot encoding\n",
        "# Isso é necessário para a camada de saída da RNN (softmax)\n",
        "saidas_y_one_hot = tf.keras.utils.to_categorical(saidas_y, num_classes=total_palavras)\n",
        "\n",
        "print(f\"\\nExemplo de entradas_X_padded (após padding e truncagem): \\n{entradas_X_padded[0:5]}\")\n",
        "print(f\"Exemplo de saidas_y_one_hot (após one-hot encoding): \\n{saidas_y_one_hot [0:5]}\")\n",
        "print(f\"Formato final das entradas (X): {entradas_X_padded. shape}\")\n",
        "print(f\"Formato final das saídas (y): {saidas_y_one_hot.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhC-StTjmCnW",
        "outputId": "dfb65d12-54eb-4a74-d94e-260386812f18"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comprimento máximo das sequências antes do padding: 6\n",
            "Exemplo de entradas_X (parcial): [[6], [6, 2], [6, 2, 3], [6, 2, 3, 4], [6, 2, 3, 4, 7]]\n",
            "Exemplo de saidas_y (parcial): [2, 3, 4, 7, 1]\n",
            "\n",
            "Exemplo de entradas_X_padded (após padding e truncagem): \n",
            "[[0 0 0 0 6]\n",
            " [0 0 0 6 2]\n",
            " [0 0 6 2 3]\n",
            " [0 6 2 3 4]\n",
            " [6 2 3 4 7]]\n",
            "Exemplo de saidas_y_one_hot (após one-hot encoding): \n",
            "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "Formato final das entradas (X): (21, 5)\n",
            "Formato final das saídas (y): (21, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 3: Construção do Modelo RNN ###"
      ],
      "metadata": {
        "id": "UowgPH97pJH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Definindo o modelo\n",
        "# Definir a arquitetura do modelo RNN\n",
        "modelo_rnn = Sequential ()\n",
        "# Camada de Embedding:\n",
        "# total_palavras: tamanho do vocabulário\n",
        "# 10: dimensão do vetor de embedding (quantas características queremos para cada palavra)\n",
        "# input_length: comprimento das sequências de entrada (maxlen - 1)\n",
        "modelo_rnn. add (Embedding(total_palavras, 10, input_length=entradas_X_padded.shape [1]))\n",
        "# Camada SimpleRNN:\n",
        "# 32: número de unidades (neurônios) na camada recorrente. Este é o tamanho do estado oculto.\n",
        "modelo_rnn.add (SimpleRNN(32))\n",
        "# Camada Densa de Saída:\n",
        "# total _palavras: número de neurônios de saída (um para cada palavra no vocabulário)\n",
        "# activation='softmax': função de ativação para probabilidade (soma 1 para todas as palavras)\n",
        "modelo_rnn. add (Dense(total_palavras, activation='softmax'))\n",
        "# Compilar o modelo\n",
        "modelo_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Exibir um resumo da arquitetura do modelo\n",
        "modelo_rnn. summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "Udv9GdgIsAJn",
        "outputId": "061eccde-9ce8-41af-b650-1954779d6602"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 4: Treinamento do Modelo ###"
      ],
      "metadata": {
        "id": "eG80xsn_pPZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinando o modelo\n",
        "print(\"\\nIniciando o treinamento do modelo RNN...\")\n",
        "modelo_rnn. fit(entradas_X_padded, saidas_y_one_hot, epochs=100, verbose=1)\n",
        "# epochs: quantas vezes o modelo verá todo o conjunto de dados\n",
        "# verbose: 1 para mostrar o progresso do treinamento\n",
        "print (\"Treinamento concluído!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1rJoGAHsFvM",
        "outputId": "0c391442-5a57-491e-ae99-959c8ad3e537"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando o treinamento do modelo RNN...\n",
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 0.0476 - loss: 2.9911\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.0476 - loss: 2.9819\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - accuracy: 0.0952 - loss: 2.9728\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.1905 - loss: 2.9637\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.1905 - loss: 2.9546\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.1905 - loss: 2.9454\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.1429 - loss: 2.9361\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - accuracy: 0.1905 - loss: 2.9265\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2857 - loss: 2.9167\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.3333 - loss: 2.9067\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3333 - loss: 2.8963\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.3333 - loss: 2.8856\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.3333 - loss: 2.8746\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3333 - loss: 2.8631\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.2857 - loss: 2.8512\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2857 - loss: 2.8388\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2857 - loss: 2.8260\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2857 - loss: 2.8127\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2857 - loss: 2.7989\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2857 - loss: 2.7847\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2857 - loss: 2.7700\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2857 - loss: 2.7550\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2857 - loss: 2.7396\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.2857 - loss: 2.7238\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.2381 - loss: 2.7078\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.2381 - loss: 2.6916\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.2381 - loss: 2.6751\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.2381 - loss: 2.6586\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.2381 - loss: 2.6419\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.2381 - loss: 2.6250\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.2381 - loss: 2.6080\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.2381 - loss: 2.5909\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.2381 - loss: 2.5735\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.2381 - loss: 2.5559\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.2381 - loss: 2.5381\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2857 - loss: 2.5199\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.2857 - loss: 2.5015\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.2857 - loss: 2.4827\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.2857 - loss: 2.4636\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.2857 - loss: 2.4441\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.2857 - loss: 2.4243\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.2857 - loss: 2.4040\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.2857 - loss: 2.3832\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.2857 - loss: 2.3619\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.2857 - loss: 2.3401\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.2857 - loss: 2.3178\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.2857 - loss: 2.2950\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.2857 - loss: 2.2718\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.2857 - loss: 2.2481\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.3333 - loss: 2.2240\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.3333 - loss: 2.1997\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.3333 - loss: 2.1751\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.3333 - loss: 2.1503\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.3333 - loss: 2.1253\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.3810 - loss: 2.1002\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.4286 - loss: 2.0749\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.5238 - loss: 2.0495\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.5238 - loss: 2.0240\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.6190 - loss: 1.9984\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6190 - loss: 1.9728\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.6190 - loss: 1.9471\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.6190 - loss: 1.9215\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.6190 - loss: 1.8959\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.6190 - loss: 1.8703\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6190 - loss: 1.8448\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6190 - loss: 1.8193\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6190 - loss: 1.7939\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6190 - loss: 1.7686\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6190 - loss: 1.7434\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6190 - loss: 1.7183\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6190 - loss: 1.6933\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6190 - loss: 1.6686\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6190 - loss: 1.6439\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6190 - loss: 1.6195\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6190 - loss: 1.5952\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6190 - loss: 1.5711\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6190 - loss: 1.5472\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6190 - loss: 1.5234\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.6667 - loss: 1.4999\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6667 - loss: 1.4766\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.6667 - loss: 1.4535\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6667 - loss: 1.4306\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6667 - loss: 1.4079\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7143 - loss: 1.3855\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.7143 - loss: 1.3633\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7619 - loss: 1.3413\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.7619 - loss: 1.3196\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7619 - loss: 1.2981\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7619 - loss: 1.2768\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7619 - loss: 1.2558\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7619 - loss: 1.2351\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7619 - loss: 1.2146\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7619 - loss: 1.1944\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8095 - loss: 1.1744\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8095 - loss: 1.1547\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8095 - loss: 1.1352\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8095 - loss: 1.1160\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8571 - loss: 1.0971\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8571 - loss: 1.0785\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8571 - loss: 1.0601\n",
            "Treinamento concluído!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 5: Usar o Modelo para Previsão ###"
      ],
      "metadata": {
        "id": "KhrPM3a0pTXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Função de Previsão:\n",
        "def prever_proxima_palavra(modelo, tokenizer, max_seq_len, texto_base):\n",
        "  \"\"\"\n",
        "  Prevê a próxima palavra dado um texto base.\n",
        "  \"\"\"\n",
        "  # Converter o texto base para sequência numérica\n",
        "  sequencia_numerica = tokenizer. texts_to_sequences ([texto_base])[0]\n",
        "\n",
        "  # Padronizar o comprimento da sequência de entrada (precisa ter o mesmo formato que o treinamento)\n",
        "  # Atenção: max seq_len deve ser o comprimento que as *entradas* foram pad_ sequenciadas\n",
        "  sequencia_padded = pad_sequences ([sequencia_numerica], maxlen=max_seq_len, padding='pre')\n",
        "\n",
        "  # Fazer a previsão\n",
        "  previsao_probabilidades = modelo.predict (sequencia_padded, verbose=0) [0]\n",
        "\n",
        "  # Obter o índice da palavra com a maior probabilidade\n",
        "  indice_palavra_prevista = np.argmax(previsao_probabilidades)\n",
        "\n",
        "  # Converter o índice de volta para a palavra\n",
        "  for palavra, indice in tokenizer.word_index.items():\n",
        "    if indice == indice_palavra_prevista:\n",
        "      return palavra\n",
        "  return None # Caso a palavra não seja encontrada (improvável com o vocabulário ajustado)\n",
        "\n",
        "  # Comprimento de entrada esperado pelo modelo\n",
        "  # entradas_X_padded. shape[1] é o maxlen que usamos no pad_sequences para X\n",
        "  comprimento_entrada_modelo = entradas_X_padded.shape [1]\n",
        "\n",
        "  # Testar o modelo com novas frases\n",
        "  print(\"\\n--- Testando o Modelo RNN ---\")\n",
        "\n",
        "  texto_teste_1 = \"eu gosto de\"\n",
        "  proxima_1 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_1)\n",
        "  print(f\"Texto: '{texto_teste_1}' -› Próxima palavra prevista: '{proxima_1}'\")\n",
        "\n",
        "  texto_teste_2 = \"python é uma\"\n",
        "  proxima_2 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_2)\n",
        "  print(f\"Texto: '{texto_teste_2}' -> Próxima palavra prevista: '{proxima_2}'\")\n",
        "\n",
        "  texto_teste_3 = \"programar é divertido\"\n",
        "  proxima_3 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_3)\n",
        "  print(f\"Texto: '{texto_teste_3}' -> Próxima palavra prevista: '{proxima_3}'\")\n",
        "\n",
        "  texto_teste_4 = \"aprenda python e\"\n",
        "  proxima_4 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_4)\n",
        "  print(f\"Texto: '{texto_teste_4}' -> Próxima palavra prevista: '{proxima_4}'\")\n",
        "\n",
        "  # Exemplo com palavra fora do vocabulário (ou sequência que o modelo nunca viu antes) texto teste 5 = \"o sol brilha no\" # Palavras \"sol\" e \"brilha\" não estão no vocabulário\n",
        "  proxima_5 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_5)\n",
        "  print(f\"Texto: '{texto_teste_5}' -> Próxima palavra prevista: '{proxima_5}' (Pode ser inesperada devido a palavras desconhecidas)\")"
      ],
      "metadata": {
        "id": "Stwvh8pIsT8m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Implementação: Modelo de Rede Neural Rede Long Short-Term Memory ###\n",
        "#### Passo 1: Configuração do Ambiente e Importação de Bibliotecas ####"
      ],
      "metadata": {
        "id": "80UOZ1M6pbhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necessárias\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvV0xFWTub3V",
        "outputId": "16c464ed-d07d-410c-80c3-47011f6728d1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Passo 2: Preparação do Conjunto de Dados de Análise de Sentimentos ####"
      ],
      "metadata": {
        "id": "gXB35p0pps-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir o Conjunto de Dados (Frases e Rótulos) para análise de sentimentos\n",
        "dados_sentimento = [\n",
        "  (\"este filme é ótimo e divertido\", \"positivo\"),\n",
        "  (\"eu adorei o livro, muito bom\", \"positivo\"),\n",
        "  (\"gostei muito da atuação dos atores\", \"positivo\"),\n",
        "  (\"o roteiro é fraco e chato\", \"negativo\"),\n",
        "  (\"não recomendo este péssimo produto\", \"negativo\"),\n",
        "  (\"uma perda de tempo horrível\", \"negativo\"),\n",
        "  (\"ótimo trabalho, parabéns\", \"positivo\"),\n",
        "  (\"terrível experiência, nunca mais\", \"negativo\"),\n",
        "  (\"excelente serviço, muito eficiente\", \"positivo\"),\n",
        "  (\"que decepção, muito ruim\", \"negativo\"),\n",
        "  (\"aprendizagem de máquina é fascinante\", \"positivo\"),\n",
        "  (\"pln é um campo interessante\", \"positivo\"),\n",
        "  (\"este software travou várias vezes\", \"negativo\"),\n",
        "  (\"a interface é confusa e difícil\", \"negativo\"),\n",
        "  (\"o aplicativo é super útil e rápido\", \"positivo\")\n",
        "]\n",
        "\n",
        "textos = [dado[0] for dado in dados_sentimento]\n",
        "sentimentos = [dado[1] for dado in dados_sentimento]\n",
        "print(f\"Total de frases: {len(textos)}\")\n",
        "print(f\"Exemplo de textos: {textos[:3]}\")\n",
        "print(f\"Exemplo de sentimentos: {sentimentos [:3]}\")\n",
        "\n",
        "# Mapear Sentimentos para Números: converter \"positivo\" e \"negativo\" para 0 e 1.\n",
        "mapeamento_sentimento = {'negativo': 0, 'positivo': 1}\n",
        "rotulos_numericos = np.array([mapeamento_sentimento[s] for s in sentimentos])\n",
        "\n",
        "print(f\"\\nSentimentos mapeados para números: {rotulos_numericos}\")\n",
        "\n",
        "# Tokenização de Texto\n",
        "tokenizer = Tokenizer (num_words=None, oov_token=\"<unk>\")\n",
        "# num words=None para pegar todo o vocabulário\n",
        "# oov_token para palavras desconhecidas\n",
        "\n",
        "tokenizer.fit_on_texts(textos)\n",
        "sequencias_numericas = tokenizer.texts_to_sequences (textos)\n",
        "total_palavras_vocab = len(tokenizer.word_index) + 1 # +1 para o ® de padding/00V\n",
        "\n",
        "print(f\"\\nVocabulário (palavra: indice): {tokenizer.word_index}\")\n",
        "print(f\"Sequências numéricas das frases: {sequencias_numericas}\")\n",
        "print(f\"Tamanho total do vocabulário: {total_palavras_vocab}\")\n",
        "\n",
        "# Padronizar o comprimento das sequências\n",
        "# Encontrar o comprimento da frase mais longa para padronizar\n",
        "max_len = max(len(s) for s in sequencias_numericas)\n",
        "print(f\"\\nComprimento máximo das sequências: {max_len}\")\n",
        "sequencias_padded = pad_sequences (sequencias_numericas, maxlen=max_len, padding='post') # 'post' para adicionar zeros no final\n",
        "print(f\"sequências após padding: \\n{sequencias_padded}\")\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
        "  sequencias_padded, rotulos_numericos, test_size=0.2, random_state=42, stratify=rotulos_numericos\n",
        ")\n",
        "\n",
        "print(f\"\\nShape de X_treino: {X_treino.shape}\")\n",
        "print (f\"shape de X_teste: {X_teste.shape}\")\n",
        "print(f\"Shape de y_treino: {y_treino.shape}\")\n",
        "print(f\"Shape de y_teste: {y_teste.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvU-_P18uzsn",
        "outputId": "9816902a-69df-4e85-e1f3-2e12543c10a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de frases: 15\n",
            "Exemplo de textos: ['este filme é ótimo e divertido', 'eu adorei o livro, muito bom', 'gostei muito da atuação dos atores']\n",
            "Exemplo de sentimentos: ['positivo', 'positivo', 'positivo']\n",
            "\n",
            "Sentimentos mapeados para números: [1 1 1 0 0 0 1 0 1 0 1 1 0 0 1]\n",
            "\n",
            "Vocabulário (palavra: indice): {'<unk>': 1, 'é': 2, 'e': 3, 'muito': 4, 'este': 5, 'o': 6, 'ótimo': 7, 'de': 8, 'filme': 9, 'divertido': 10, 'eu': 11, 'adorei': 12, 'livro': 13, 'bom': 14, 'gostei': 15, 'da': 16, 'atuação': 17, 'dos': 18, 'atores': 19, 'roteiro': 20, 'fraco': 21, 'chato': 22, 'não': 23, 'recomendo': 24, 'péssimo': 25, 'produto': 26, 'uma': 27, 'perda': 28, 'tempo': 29, 'horrível': 30, 'trabalho': 31, 'parabéns': 32, 'terrível': 33, 'experiência': 34, 'nunca': 35, 'mais': 36, 'excelente': 37, 'serviço': 38, 'eficiente': 39, 'que': 40, 'decepção': 41, 'ruim': 42, 'aprendizagem': 43, 'máquina': 44, 'fascinante': 45, 'pln': 46, 'um': 47, 'campo': 48, 'interessante': 49, 'software': 50, 'travou': 51, 'várias': 52, 'vezes': 53, 'a': 54, 'interface': 55, 'confusa': 56, 'difícil': 57, 'aplicativo': 58, 'super': 59, 'útil': 60, 'rápido': 61}\n",
            "Sequências numéricas das frases: [[5, 9, 2, 7, 3, 10], [11, 12, 6, 13, 4, 14], [15, 4, 16, 17, 18, 19], [6, 20, 2, 21, 3, 22], [23, 24, 5, 25, 26], [27, 28, 8, 29, 30], [7, 31, 32], [33, 34, 35, 36], [37, 38, 4, 39], [40, 41, 4, 42], [43, 8, 44, 2, 45], [46, 2, 47, 48, 49], [5, 50, 51, 52, 53], [54, 55, 2, 56, 3, 57], [6, 58, 2, 59, 60, 3, 61]]\n",
            "Tamanho total do vocabulário: 62\n",
            "\n",
            "Comprimento máximo das sequências: 7\n",
            "sequências após padding: \n",
            "[[ 5  9  2  7  3 10  0]\n",
            " [11 12  6 13  4 14  0]\n",
            " [15  4 16 17 18 19  0]\n",
            " [ 6 20  2 21  3 22  0]\n",
            " [23 24  5 25 26  0  0]\n",
            " [27 28  8 29 30  0  0]\n",
            " [ 7 31 32  0  0  0  0]\n",
            " [33 34 35 36  0  0  0]\n",
            " [37 38  4 39  0  0  0]\n",
            " [40 41  4 42  0  0  0]\n",
            " [43  8 44  2 45  0  0]\n",
            " [46  2 47 48 49  0  0]\n",
            " [ 5 50 51 52 53  0  0]\n",
            " [54 55  2 56  3 57  0]\n",
            " [ 6 58  2 59 60  3 61]]\n",
            "\n",
            "Shape de X_treino: (12, 7)\n",
            "shape de X_teste: (3, 7)\n",
            "Shape de y_treino: (12,)\n",
            "Shape de y_teste: (3,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Passo 3: Construção do Modelo LSTM ####"
      ],
      "metadata": {
        "id": "DOceMu7fp1qE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir a arquitetura do modelo LST™\n",
        "modelo_lstm = Sequential()\n",
        "\n",
        "# Camada de Embedding: Converte os índices numéricos das palavras em vetores densos.\n",
        "# total_palavras _vocab: tamanho do vocabulário\n",
        "# 50: dimensão do vetor de embedding (pode ser ajustado)\n",
        "# input_length: comprimento padronizado das sequências (max_len)\n",
        "modelo_lstm.add (Embedding(total_palavras_vocab, 50, input_length=max_len))\n",
        "\n",
        "# Camada LSTM:\n",
        "# 64: número de unidades (neurônios) na camada LSTM. Define o tamanho do estado oculto e da célula de memória.\n",
        "# dropou: Um tipo de regularização para evitar overfitting (descarta aleatoriamente neurônios durante o treinamento) .\n",
        "# recurrent_dropout: Dropout aplicado nas conexões recorrentes da LSTM.\n",
        "modelo_lstm.add (LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# Camada Densa de Saida:\n",
        "# 1: Um único neurônio de saída, pois é um problema de classificação binária (positivo/negativo).\n",
        "# activation='sigmoid': Função de ativação para classificação binária (produz um valor entre 0 e 1).\n",
        "modelo_lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compilar o modelo\n",
        "modelo_lstm. compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Exibir um resumo da arquitetura do modelo\n",
        "modelo_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "uBcLyjn3xDoS",
        "outputId": "fcf672ac-f62b-4a71-ae9b-a43edefc49ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Passo 4: Treinamento e Avaliação do Modelo ####"
      ],
      "metadata": {
        "id": "8smiCsfzp--N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar o modelo\n",
        "print(\"\\nIniciando o treinamento do modelo LSTM...\")\n",
        "historico = modelo_lstm.fit(\n",
        "    X_treino, y_treino,\n",
        "    epochs=50,\n",
        "    batch_size=2,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Treinamento concluído!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLd1zUmXxUm3",
        "outputId": "554280ca-5a40-4cbd-c094-888c37b59812"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando o treinamento do modelo LSTM...\n",
            "Epoch 1/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 230ms/step - accuracy: 0.5417 - loss: 0.6940 - val_accuracy: 0.5000 - val_loss: 0.6961\n",
            "Epoch 2/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6236 - loss: 0.6899 - val_accuracy: 0.5000 - val_loss: 0.6978\n",
            "Epoch 3/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5556 - loss: 0.6862 - val_accuracy: 0.5000 - val_loss: 0.6992\n",
            "Epoch 4/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7931 - loss: 0.6840 - val_accuracy: 0.5000 - val_loss: 0.7008\n",
            "Epoch 5/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.6733 - val_accuracy: 0.5000 - val_loss: 0.7055\n",
            "Epoch 6/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.6614 - val_accuracy: 0.5000 - val_loss: 0.7125\n",
            "Epoch 7/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.6376 - val_accuracy: 0.5000 - val_loss: 0.7277\n",
            "Epoch 8/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.5990 - val_accuracy: 0.0000e+00 - val_loss: 0.7546\n",
            "Epoch 9/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.5531 - val_accuracy: 0.0000e+00 - val_loss: 0.8134\n",
            "Epoch 10/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.4142 - val_accuracy: 0.0000e+00 - val_loss: 0.9357\n",
            "Epoch 11/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 0.2362 - val_accuracy: 0.0000e+00 - val_loss: 1.2260\n",
            "Epoch 12/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.0924 - val_accuracy: 0.0000e+00 - val_loss: 1.7706\n",
            "Epoch 13/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0425 - val_accuracy: 0.0000e+00 - val_loss: 2.5015\n",
            "Epoch 14/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.0122 - val_accuracy: 0.0000e+00 - val_loss: 3.1509\n",
            "Epoch 15/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.0116 - val_accuracy: 0.0000e+00 - val_loss: 3.6408\n",
            "Epoch 16/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 0.0000e+00 - val_loss: 3.9707\n",
            "Epoch 17/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.0000e+00 - val_loss: 4.2045\n",
            "Epoch 18/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.0000e+00 - val_loss: 4.3674\n",
            "Epoch 19/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.0000e+00 - val_loss: 4.4824\n",
            "Epoch 20/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.0000e+00 - val_loss: 4.5769\n",
            "Epoch 21/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.0000e+00 - val_loss: 4.6533\n",
            "Epoch 22/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.0000e+00 - val_loss: 4.7218\n",
            "Epoch 23/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.0000e+00 - val_loss: 4.7808\n",
            "Epoch 24/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.0000e+00 - val_loss: 4.8337\n",
            "Epoch 25/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.0000e+00 - val_loss: 4.8859\n",
            "Epoch 26/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.0000e+00 - val_loss: 4.9344\n",
            "Epoch 27/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.0000e+00 - val_loss: 4.9858\n",
            "Epoch 28/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.0000e+00 - val_loss: 5.0375\n",
            "Epoch 29/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.0000e+00 - val_loss: 5.0878\n",
            "Epoch 30/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.0000e+00 - val_loss: 5.1327\n",
            "Epoch 31/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.0000e+00 - val_loss: 5.1800\n",
            "Epoch 32/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.0000e+00 - val_loss: 5.2284\n",
            "Epoch 33/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.0000e+00 - val_loss: 5.2732\n",
            "Epoch 34/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.0000e+00 - val_loss: 5.3189\n",
            "Epoch 35/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.0000e+00 - val_loss: 5.3597\n",
            "Epoch 36/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 9.6406e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.3979\n",
            "Epoch 37/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.0000e+00 - val_loss: 5.4352\n",
            "Epoch 38/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.0000e+00 - val_loss: 5.4737\n",
            "Epoch 39/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 8.2240e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.5119\n",
            "Epoch 40/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.0000e+00 - val_loss: 5.5505\n",
            "Epoch 41/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.0000e+00 - val_loss: 5.5904\n",
            "Epoch 42/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 7.4066e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.6278\n",
            "Epoch 43/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 9.6952e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.6664\n",
            "Epoch 44/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 7.4358e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.7035\n",
            "Epoch 45/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 8.3261e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.7389\n",
            "Epoch 46/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 7.7546e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.7732\n",
            "Epoch 47/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 7.6049e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.8092\n",
            "Epoch 48/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 6.7655e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.8438\n",
            "Epoch 49/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 5.1423e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.8759\n",
            "Epoch 50/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 6.0087e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.9071\n",
            "Treinamento concluído!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Avalia o modelo no conjunto de teste\n",
        "perda, acuracia = modelo_lstm.evaluate(X_teste, y_teste, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia do modelo no conjunto de teste: {acuracia*100:.2f}%\")\n",
        "print(f\"Perda do modelo no conjunto de teste: {perda: .4f}\")\n",
        "\n",
        "# Fazer previsões no conjunto de teste\n",
        "y_pred_prob = modelo_lstm.predict(X_teste)\n",
        "y_pred_classes = (y_pred_prob > 0.5).astype(int) # Converter probabilidades para 0 ou 1\n",
        "print(\"\\n--- Relatório de Classificação ---\")\n",
        "print(classification_report(y_teste, y_pred_classes, target_names=['negativo', 'positivo']))\n",
        "print(\"\\n--- Matriz de Confusão ---\")\n",
        "cm = confusion_matrix(y_teste, y_pred_classes)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[ 'negativo', 'positivo'], yticklabels=['negativo', 'positivo'])\n",
        "plt.xlabel('Previsto')\n",
        "plt.ylabel ('Real')\n",
        "plt.title('Matriz de Confusão')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "QTxZUbAFx-h0",
        "outputId": "c4aa8626-695a-4b11-ab7e-d418becb92bc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acurácia do modelo no conjunto de teste: 33.33%\n",
            "Perda do modelo no conjunto de teste:  2.2664\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 992ms/step\n",
            "\n",
            "--- Relatório de Classificação ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo       0.00      0.00      0.00         1\n",
            "    positivo       0.50      0.50      0.50         2\n",
            "\n",
            "    accuracy                           0.33         3\n",
            "   macro avg       0.25      0.25      0.25         3\n",
            "weighted avg       0.33      0.33      0.33         3\n",
            "\n",
            "\n",
            "--- Matriz de Confusão ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Passo 5: Testar o Modelo com Novas Frases ####"
      ],
      "metadata": {
        "id": "bKr153zIqD1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utilizando o modelo treinado\n",
        "def prever_sentimento(modelo, tokenizer, max_seq_len, frase_nova, mapeamento_sentimento):\n",
        "  \"\"\"\"\n",
        "  Prevê o sentimento de uma nova frase.\n",
        "  \"\"\"\n",
        "  # Converter a frase para sequência numérica\n",
        "  sequencia_numerica = tokenizer.texts_to_sequences([frase_nova])\n",
        "\n",
        "  # Se a frase tem palavras desconhecidas, o tokenizer pode retornar uma lista vazia ou valores 0\n",
        "  if not sequencia_numerica:\n",
        "    print(f\"Aviso: A frase '{frase_nova}' contém apenas palavras desconhecidas.\")\n",
        "    return \"Desconhecido\" # Ou outra indicação\n",
        "\n",
        "  sequencia_numerica = sequencia_numerica[0] # Pega a primeira (e única) sequência\n",
        "\n",
        "  # Padronizar o comprimento da sequência de entrada\n",
        "  sequencia_padded = pad_sequences ([sequencia_numerica], maxlen=max_seq_len, padding= 'post')\n",
        "\n",
        "  # Fazer a previsão (probabilidade)\n",
        "  probabilidade_positiva = modelo.predict(sequencia_padded, verbose=0) [0][0]\n",
        "\n",
        "  # Inverter o mapeamento para obter o nome do sentimento\n",
        "  mapeamento_inverso = {v: k for k, v in mapeamento_sentimento.items()}\n",
        "\n",
        "  # Classificar com base no limiar de 0.5\n",
        "  if probabilidade_positiva >= 0.5:\n",
        "    return mapeamento_inverso[1] # 'positivo'\n",
        "  else:\n",
        "    return mapeamento_inverso[0] # 'negativo'\n",
        "\n",
        "# Testar o modelo com novas frases\n",
        "print(\"\\n--- Testando o Modelo LSTM com Novas Frases ---\")\n",
        "\n",
        "frase_nova_1 = \"gostei muito do filme, excelente!\"\n",
        "sentimento_1 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_1, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_1}' -> Sentimento previsto: '{sentimento_1}'\")\n",
        "\n",
        "frase_nova_2 = \"odiei o livro, muito entediante\"\n",
        "sentimento_2 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_2, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_2}' -> Sentimento previsto: '{sentimento_2}'\")\n",
        "\n",
        "frase_nova_3 = \"a aula de pin é ótima\"\n",
        "sentimento_3 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_3, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_3}' -> Sentimento previsto: '{sentimento_3}'\")\n",
        "\n",
        "frase_nova_4 = \"o atendimento foi péssimo\"\n",
        "sentimento_4 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_4, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_4}' -> Sentimento previsto: '{sentimento_4}'\")\n",
        "\n",
        "frase_nova_5 = \"esse produto não vale a pena, é caro\"\n",
        "sentimento_5 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_5, mapeamento_sentimento)\n",
        "print(f\"Frase: '(frase_nova_5)' -> Sentimento previsto: '{sentimento_5}'\")\n",
        "\n",
        "frase_nova_6 = \"o filme é legal\" # Frase curta e ambígua para um modelo pequeno\n",
        "sentimento_6 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_6, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_6}' -> Sentimento previsto: '{sentimento_6}'\")\n",
        "\n",
        "frase_nova_7 = \"isso é horrível, que tristeza\"\n",
        "sentimento_7 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_7, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_7}' -> Sentimento previsto: '{sentimento_7}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dpNXnDkqKaY",
        "outputId": "5b3ebedb-9890-44f1-beb8-e63b39f1476e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testando o Modelo LSTM com Novas Frases ---\n",
            "Frase: 'gostei muito do filme, excelente!' -> Sentimento previsto: 'positivo'\n",
            "Frase: 'odiei o livro, muito entediante' -> Sentimento previsto: 'positivo'\n",
            "Frase: 'a aula de pin é ótima' -> Sentimento previsto: 'positivo'\n",
            "Frase: 'o atendimento foi péssimo' -> Sentimento previsto: 'negativo'\n",
            "Frase: '(frase_nova_5)' -> Sentimento previsto: 'negativo'\n",
            "Frase: 'o filme é legal' -> Sentimento previsto: 'negativo'\n",
            "Frase: 'isso é horrível, que tristeza' -> Sentimento previsto: 'positivo'\n"
          ]
        }
      ]
    }
  ]
}
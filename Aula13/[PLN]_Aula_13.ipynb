{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Aula 13** - Redes Neurais em PLN\n",
        "## Objetivos: ##\n",
        "- Implementar uma Rede Neural Recorrente (RNN) simples em Python para prever a próxima palavra em uma sequência de texto, utilizando a biblioteca TensorFlow/Keras.\n",
        "- Implementar uma Rede Long Short-Term Memory (LSTM) em Python para classificar o sentimento de frases como \"positivo\" ou \"negativo\", utilizando a biblioteca TensorFlow/Keras.\n"
      ],
      "metadata": {
        "id": "NkQlPoJw0Roy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 1: Configuração do Ambiente no Google Colab ###\n"
      ],
      "metadata": {
        "id": "7H1vLvSHpTRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necessárias\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "print(\"Bibliotecas importadas com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fv64b4SpzUs",
        "outputId": "4af63b27-659a-4597-ebec-3da8a9956f67"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 2: Preparação do Conjunto de Dados ###"
      ],
      "metadata": {
        "id": "IVD9UAYto_mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conjunto de dados de treinamento (pequeno e simplificado)\n",
        "textos_treinamento = [\n",
        "\"eu gosto de programar em python\",\n",
        "\"python é uma linguagem poderosa\",\n",
        "\"programar é divertido com python\",\n",
        "\"aprenda python e seja feliz\",\n",
        "\"gosto de aprender coisas novas\"\n",
        "]\n",
        "\n",
        "print(f\"Textos de treinamento: {textos_treinamento}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRWN6HeNlZQW",
        "outputId": "d0ba9b0a-a4b5-4c36-e3e3-2bf5683ca1c2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textos de treinamento: ['eu gosto de programar em python', 'python é uma linguagem poderosa', 'programar é divertido com python', 'aprenda python e seja feliz', 'gosto de aprender coisas novas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar o Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# Construir o vocabulário a partir dos textos\n",
        "tokenizer. fit_on_texts(textos_treinamento)\n",
        "# Converter textos em sequências de números\n",
        "sequencias = tokenizer.texts_to_sequences(textos_treinamento)\n",
        "# Imprimir o vocabulário e as sequências geradas print(f\"\\nVocabulário (palavra: índice): (tokenizer.word_index}*)\n",
        "print (f\"Sequências numéricas dos textos: {sequencias}\")\n",
        "# Calcular o tamanho do vocabulário (+1 para incluir o 0 de padding)\n",
        "total_palavras = len(tokenizer.word_index) + 1\n",
        "print(f\"Tamanho total do vocabulário: {total_palavras}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiueZHXzljBQ",
        "outputId": "c654aef8-d0f9-4acf-9ec0-2030bbc770c0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequências numéricas dos textos: [[6, 2, 3, 4, 7, 1], [1, 5, 8, 9, 10], [4, 5, 11, 12, 1], [13, 1, 14, 15, 16], [2, 3, 17, 18, 19]]\n",
            "Tamanho total do vocabulário: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar Entradas (X) e Saidas (y) para a previsão da proxima palavra\n",
        "# a entrada (X) será uma sequência de palavras, e a saída (y) será a palavra seguinte.\n",
        "# Determinar o comprimento máximo das sequências para padding\n",
        "max_comprimento = max(len(seq) for seq in sequencias)\n",
        "print(f\"\\nComprimento máximo das sequências antes do padding: {max_comprimento}\")\n",
        "\n",
        "# Criar pares de entrada (sequência parcial) e saída (próxima palavra)\n",
        "# Ex: \"eu gosto de programar\" -> \"em\"\n",
        "# \"gosto de programar em\" -> \"python\"\n",
        "entradas_X = []\n",
        "saidas_y = []\n",
        "\n",
        "for seq in sequencias:\n",
        "  for i in range(1, len(seq)) :\n",
        "    entradas_X.append(seq[:i]) # A sequência até a palavra atual\n",
        "    saidas_y.append(seq[i]) # A proxima palavra\n",
        "\n",
        "print(f\"Exemplo de entradas_X (parcial): {entradas_X[0:5]}\")\n",
        "print(f\"Exemplo de saidas_y (parcial): {saidas_y[0:5]}\")\n",
        "\n",
        "# Padronizar o comprimento das sequências de entrada\n",
        "# Todas as sequências de entrada precisam ter o mesmo comprimento para a RNN\n",
        "entradas_X_padded = pad_sequences (entradas_X, maxlen=max_comprimento -1, padding='pre')\n",
        "\n",
        "# O maxlen é \"max_comprimento - 1' porque a saída 'y' é a última palavra, então x sempre terá 1 palavra a menos.\n",
        "# Converter as saídas para o formato one-hot encoding\n",
        "# Isso é necessário para a camada de saída da RNN (softmax)\n",
        "saidas_y_one_hot = tf.keras.utils.to_categorical(saidas_y, num_classes=total_palavras)\n",
        "\n",
        "print(f\"\\nExemplo de entradas_X_padded (após padding e truncagem): \\n{entradas_X_padded[0:5]}\")\n",
        "print(f\"Exemplo de saidas_y_one_hot (após one-hot encoding): \\n{saidas_y_one_hot [0:5]}\")\n",
        "print(f\"Formato final das entradas (X): {entradas_X_padded. shape}\")\n",
        "print(f\"Formato final das saídas (y): {saidas_y_one_hot.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhC-StTjmCnW",
        "outputId": "ccb1a3ec-fa0f-4285-bb3c-2e98f50a1532"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comprimento máximo das sequências antes do padding: 6\n",
            "Exemplo de entradas_X (parcial): [[6], [6, 2], [6, 2, 3], [6, 2, 3, 4], [6, 2, 3, 4, 7]]\n",
            "Exemplo de saidas_y (parcial): [2, 3, 4, 7, 1]\n",
            "\n",
            "Exemplo de entradas_X_padded (após padding e truncagem): \n",
            "[[0 0 0 0 6]\n",
            " [0 0 0 6 2]\n",
            " [0 0 6 2 3]\n",
            " [0 6 2 3 4]\n",
            " [6 2 3 4 7]]\n",
            "Exemplo de saidas_y_one_hot (após one-hot encoding): \n",
            "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "Formato final das entradas (X): (21, 5)\n",
            "Formato final das saídas (y): (21, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 3: Construção do Modelo RNN ###"
      ],
      "metadata": {
        "id": "UowgPH97pJH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Definindo o modelo\n",
        "# Definir a arquitetura do modelo RNN\n",
        "modelo_rnn = Sequential ()\n",
        "# Camada de Embedding:\n",
        "# total_palavras: tamanho do vocabulário\n",
        "# 10: dimensão do vetor de embedding (quantas características queremos para cada palavra)\n",
        "# input_length: comprimento das sequências de entrada (maxlen - 1)\n",
        "modelo_rnn. add (Embedding(total_palavras, 10, input_length=entradas_X_padded.shape [1]))\n",
        "# Camada SimpleRNN:\n",
        "# 32: número de unidades (neurônios) na camada recorrente. Este é o tamanho do estado oculto.\n",
        "modelo_rnn.add (SimpleRNN(32))\n",
        "# Camada Densa de Saída:\n",
        "# total _palavras: número de neurônios de saída (um para cada palavra no vocabulário)\n",
        "# activation='softmax': função de ativação para probabilidade (soma 1 para todas as palavras)\n",
        "modelo_rnn. add (Dense(total_palavras, activation='softmax'))\n",
        "# Compilar o modelo\n",
        "modelo_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Exibir um resumo da arquitetura do modelo\n",
        "modelo_rnn. summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "Udv9GdgIsAJn",
        "outputId": "89a0d718-0bf3-4faf-f893-46d039c932a1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 4: Treinamento do Modelo ###"
      ],
      "metadata": {
        "id": "eG80xsn_pPZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinando o modelo\n",
        "print(\"\\nIniciando o treinamento do modelo RNN...\")\n",
        "modelo_rnn. fit(entradas_X_padded, saidas_y_one_hot, epochs=100, verbose=1)\n",
        "# epochs: quantas vezes o modelo verá todo o conjunto de dados\n",
        "# verbose: 1 para mostrar o progresso do treinamento\n",
        "print (\"Treinamento concluído!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1rJoGAHsFvM",
        "outputId": "8c266e7a-2337-45e1-cbf0-64bcf8fb8723"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando o treinamento do modelo RNN...\n",
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 2.9920\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.0476 - loss: 2.9830\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1429 - loss: 2.9739\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1429 - loss: 2.9648\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.1905 - loss: 2.9555\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.2381 - loss: 2.9461\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2381 - loss: 2.9364\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.2381 - loss: 2.9265\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2381 - loss: 2.9163\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.1905 - loss: 2.9057\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.1905 - loss: 2.8948\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1905 - loss: 2.8835\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.1905 - loss: 2.8718\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1905 - loss: 2.8597\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1905 - loss: 2.8471\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1905 - loss: 2.8341\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1905 - loss: 2.8206\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.1905 - loss: 2.8068\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1905 - loss: 2.7925\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1905 - loss: 2.7779\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1905 - loss: 2.7630\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1905 - loss: 2.7479\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1905 - loss: 2.7327\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1905 - loss: 2.7174\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1905 - loss: 2.7021\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1905 - loss: 2.6869\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1905 - loss: 2.6717\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.1905 - loss: 2.6565\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1905 - loss: 2.6413\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.1905 - loss: 2.6260\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1905 - loss: 2.6105\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1905 - loss: 2.5945\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1905 - loss: 2.5781\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1905 - loss: 2.5611\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1905 - loss: 2.5434\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1905 - loss: 2.5251\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.2381 - loss: 2.5061\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.2381 - loss: 2.4866\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2857 - loss: 2.4665\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2857 - loss: 2.4460\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2857 - loss: 2.4250\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2857 - loss: 2.4037\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.2857 - loss: 2.3821\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.2857 - loss: 2.3602\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.2857 - loss: 2.3381\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.2857 - loss: 2.3157\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.3333 - loss: 2.2931\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.3333 - loss: 2.2702\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.3333 - loss: 2.2471\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3333 - loss: 2.2237\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.3333 - loss: 2.2002\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.3333 - loss: 2.1764\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.3333 - loss: 2.1524\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.3333 - loss: 2.1283\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.3810 - loss: 2.1041\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.3333 - loss: 2.0798\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.3333 - loss: 2.0555\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.3810 - loss: 2.0312\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.4286 - loss: 2.0069\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5238 - loss: 1.9825\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5238 - loss: 1.9582\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5238 - loss: 1.9338\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.5238 - loss: 1.9095\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5238 - loss: 1.8851\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5238 - loss: 1.8607\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5238 - loss: 1.8362\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5238 - loss: 1.8118\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5238 - loss: 1.7873\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5238 - loss: 1.7629\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5238 - loss: 1.7384\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.5238 - loss: 1.7140\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5714 - loss: 1.6896\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6190 - loss: 1.6652\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6190 - loss: 1.6409\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6190 - loss: 1.6166\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6190 - loss: 1.5924\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6190 - loss: 1.5683\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6190 - loss: 1.5442\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6667 - loss: 1.5203\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7143 - loss: 1.4965\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.7143 - loss: 1.4729\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7143 - loss: 1.4494\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7143 - loss: 1.4260\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7143 - loss: 1.4029\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7143 - loss: 1.3800\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7619 - loss: 1.3573\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7619 - loss: 1.3348\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8095 - loss: 1.3126\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8095 - loss: 1.2906\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8095 - loss: 1.2689\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8095 - loss: 1.2475\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8095 - loss: 1.2264\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8095 - loss: 1.2056\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8095 - loss: 1.1851\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8095 - loss: 1.1649\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8095 - loss: 1.1450\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8095 - loss: 1.1254\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8095 - loss: 1.1062\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8095 - loss: 1.0873\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8095 - loss: 1.0687\n",
            "Treinamento concluído!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 5: Usar o Modelo para Previsão ###"
      ],
      "metadata": {
        "id": "KhrPM3a0pTXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Função de Previsão:\n",
        "def prever_proxima_palavra(modelo, tokenizer, max_seq_len, texto_base):\n",
        "  \"\"\"\n",
        "  Prevê a próxima palavra dado um texto base.\n",
        "  \"\"\"\n",
        "  # Converter o texto base para sequência numérica\n",
        "  sequencia_numerica = tokenizer. texts_to_sequences ([texto_base])[0]\n",
        "\n",
        "  # Padronizar o comprimento da sequência de entrada (precisa ter o mesmo formato que o treinamento)\n",
        "  # Atenção: max seq_len deve ser o comprimento que as *entradas* foram pad_ sequenciadas\n",
        "  sequencia_padded = pad_sequences ([sequencia_numerica], maxlen=max_seq_len, padding='pre')\n",
        "\n",
        "  # Fazer a previsão\n",
        "  previsao_probabilidades = modelo.predict (sequencia_padded, verbose=0) [0]\n",
        "\n",
        "  # Obter o índice da palavra com a maior probabilidade\n",
        "  indice_palavra_prevista = np.argmax(previsao_probabilidades)\n",
        "\n",
        "  # Converter o índice de volta para a palavra\n",
        "  for palavra, indice in tokenizer.word_index.items():\n",
        "    if indice == indice_palavra_prevista:\n",
        "      return palavra\n",
        "  return None # Caso a palavra não seja encontrada (improvável com o vocabulário ajustado)\n",
        "\n",
        "  # Comprimento de entrada esperado pelo modelo\n",
        "  # entradas_X_padded. shape[1] é o maxlen que usamos no pad_sequences para X\n",
        "  comprimento_entrada_modelo = entradas_X_padded.shape [1]\n",
        "\n",
        "  # Testar o modelo com novas frases\n",
        "  print(\"\\n--- Testando o Modelo RNN ---\")\n",
        "\n",
        "  texto_teste_1 = \"eu gosto de\"\n",
        "  proxima_1 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_1)\n",
        "  print(f\"Texto: '{texto_teste_1}' -› Próxima palavra prevista: '{proxima_1}'\")\n",
        "\n",
        "  texto_teste_2 = \"python é uma\"\n",
        "  proxima_2 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_2)\n",
        "  print(f\"Texto: '{texto_teste_2}' -> Próxima palavra prevista: '{proxima_2}'\")\n",
        "\n",
        "  texto_teste_3 = \"programar é divertido\"\n",
        "  proxima_3 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_3)\n",
        "  print(f\"Texto: '{texto_teste_3}' -> Próxima palavra prevista: '{proxima_3}'\")\n",
        "\n",
        "  texto_teste_4 = \"aprenda python e\"\n",
        "  proxima_4 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_4)\n",
        "  print(f\"Texto: '{texto_teste_4}' -> Próxima palavra prevista: '{proxima_4}'\")\n",
        "\n",
        "  # Exemplo com palavra fora do vocabulário (ou sequência que o modelo nunca viu antes) texto teste 5 = \"o sol brilha no\" # Palavras \"sol\" e \"brilha\" não estão no vocabulário\n",
        "  proxima_5 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_5)\n",
        "  print(f\"Texto: '{texto_teste_5}' -> Próxima palavra prevista: '{proxima_5}' (Pode ser inesperada devido a palavras desconhecidas)\")"
      ],
      "metadata": {
        "id": "Stwvh8pIsT8m"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Implementação: Modelo de Rede Neural Rede Long Short-Term Memory ###\n",
        "#### Passo 1: Configuração do Ambiente e Importação de Bibliotecas ####"
      ],
      "metadata": {
        "id": "80UOZ1M6pbhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necessárias\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvV0xFWTub3V",
        "outputId": "4f486b6d-2038-4555-b19a-7a29053d2434"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Passo 2: Preparação do Conjunto de Dados de Análise de Sentimentos ####"
      ],
      "metadata": {
        "id": "gXB35p0pps-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir o Conjunto de Dados (Frases e Rótulos) para análise de sentimentos\n",
        "dados_sentimento = [\n",
        "  (\"este filme é ótimo e divertido\", \"positivo\"),\n",
        "  (\"eu adorei o livro, muito bom\", \"positivo\"),\n",
        "  (\"gostei muito da atuação dos atores\", \"positivo\"),\n",
        "  (\"o roteiro é fraco e chato\", \"negativo\"),\n",
        "  (\"não recomendo este péssimo produto\", \"negativo\"),\n",
        "  (\"uma perda de tempo horrível\", \"negativo\"),\n",
        "  (\"ótimo trabalho, parabéns\", \"positivo\"),\n",
        "  (\"terrível experiência, nunca mais\", \"negativo\"),\n",
        "  (\"excelente serviço, muito eficiente\", \"positivo\"),\n",
        "  (\"que decepção, muito ruim\", \"negativo\"),\n",
        "  (\"aprendizagem de máquina é fascinante\", \"positivo\"),\n",
        "  (\"pln é um campo interessante\", \"positivo\"),\n",
        "  (\"este software travou várias vezes\", \"negativo\"),\n",
        "  (\"a interface é confusa e difícil\", \"negativo\"),\n",
        "  (\"o aplicativo é super útil e rápido\", \"positivo\")\n",
        "]\n",
        "\n",
        "textos = [dado[0] for dado in dados_sentimento]\n",
        "sentimentos = [dado[1] for dado in dados_sentimento]\n",
        "print(f\"Total de frases: {len(textos)}\")\n",
        "print(f\"Exemplo de textos: {textos[:3]}\")\n",
        "print(f\"Exemplo de sentimentos: {sentimentos [:3]}\")\n",
        "\n",
        "# Mapear Sentimentos para Números: converter \"positivo\" e \"negativo\" para 0 e 1.\n",
        "mapeamento_sentimento = {'negativo': 0, 'positivo': 1}\n",
        "rotulos_numericos = np.array([mapeamento_sentimento[s] for s in sentimentos])\n",
        "\n",
        "print(f\"\\nSentimentos mapeados para números: {rotulos_numericos}\")\n",
        "\n",
        "# Tokenização de Texto\n",
        "tokenizer = Tokenizer (num_words=None, oov_token=\"<unk>\")\n",
        "# num words=None para pegar todo o vocabulário\n",
        "# oov_token para palavras desconhecidas\n",
        "\n",
        "tokenizer.fit_on_texts(textos)\n",
        "sequencias_numericas = tokenizer.texts_to_sequences (textos)\n",
        "total_palavras_vocab = len(tokenizer.word_index) + 1 # +1 para o ® de padding/00V\n",
        "\n",
        "print(f\"\\nVocabulário (palavra: indice): {tokenizer.word_index}\")\n",
        "print(f\"Sequências numéricas das frases: {sequencias_numericas}\")\n",
        "print(f\"Tamanho total do vocabulário: {total_palavras_vocab}\")\n",
        "\n",
        "# Padronizar o comprimento das sequências\n",
        "# Encontrar o comprimento da frase mais longa para padronizar\n",
        "max_len = max(len(s) for s in sequencias_numericas)\n",
        "print(f\"\\nComprimento máximo das sequências: {max_len}\")\n",
        "sequencias_padded = pad_sequences (sequencias_numericas, maxlen=max_len, padding='post') # 'post' para adicionar zeros no final\n",
        "print(f\"sequências após padding: \\n{sequencias_padded}\")\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
        "  sequencias_padded, rotulos_numericos, test_size=0.2, random_state=42, stratify=rotulos_numericos\n",
        ")\n",
        "\n",
        "print(f\"\\nShape de X_treino: {X_treino.shape}\")\n",
        "print (f\"shape de X_teste: {X_teste.shape}\")\n",
        "print(f\"Shape de y_treino: {y_treino.shape}\")\n",
        "print(f\"Shape de y_teste: {y_teste.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvU-_P18uzsn",
        "outputId": "e22ad7b0-476c-4b0b-bb1b-07ecee47e01c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de frases: 15\n",
            "Exemplo de textos: ['este filme é ótimo e divertido', 'eu adorei o livro, muito bom', 'gostei muito da atuação dos atores']\n",
            "Exemplo de sentimentos: ['positivo', 'positivo', 'positivo']\n",
            "\n",
            "Sentimentos mapeados para números: [1 1 1 0 0 0 1 0 1 0 1 1 0 0 1]\n",
            "\n",
            "Vocabulário (palavra: indice): {'<unk>': 1, 'é': 2, 'e': 3, 'muito': 4, 'este': 5, 'o': 6, 'ótimo': 7, 'de': 8, 'filme': 9, 'divertido': 10, 'eu': 11, 'adorei': 12, 'livro': 13, 'bom': 14, 'gostei': 15, 'da': 16, 'atuação': 17, 'dos': 18, 'atores': 19, 'roteiro': 20, 'fraco': 21, 'chato': 22, 'não': 23, 'recomendo': 24, 'péssimo': 25, 'produto': 26, 'uma': 27, 'perda': 28, 'tempo': 29, 'horrível': 30, 'trabalho': 31, 'parabéns': 32, 'terrível': 33, 'experiência': 34, 'nunca': 35, 'mais': 36, 'excelente': 37, 'serviço': 38, 'eficiente': 39, 'que': 40, 'decepção': 41, 'ruim': 42, 'aprendizagem': 43, 'máquina': 44, 'fascinante': 45, 'pln': 46, 'um': 47, 'campo': 48, 'interessante': 49, 'software': 50, 'travou': 51, 'várias': 52, 'vezes': 53, 'a': 54, 'interface': 55, 'confusa': 56, 'difícil': 57, 'aplicativo': 58, 'super': 59, 'útil': 60, 'rápido': 61}\n",
            "Sequências numéricas das frases: [[5, 9, 2, 7, 3, 10], [11, 12, 6, 13, 4, 14], [15, 4, 16, 17, 18, 19], [6, 20, 2, 21, 3, 22], [23, 24, 5, 25, 26], [27, 28, 8, 29, 30], [7, 31, 32], [33, 34, 35, 36], [37, 38, 4, 39], [40, 41, 4, 42], [43, 8, 44, 2, 45], [46, 2, 47, 48, 49], [5, 50, 51, 52, 53], [54, 55, 2, 56, 3, 57], [6, 58, 2, 59, 60, 3, 61]]\n",
            "Tamanho total do vocabulário: 62\n",
            "\n",
            "Comprimento máximo das sequências: 7\n",
            "sequências após padding: \n",
            "[[ 5  9  2  7  3 10  0]\n",
            " [11 12  6 13  4 14  0]\n",
            " [15  4 16 17 18 19  0]\n",
            " [ 6 20  2 21  3 22  0]\n",
            " [23 24  5 25 26  0  0]\n",
            " [27 28  8 29 30  0  0]\n",
            " [ 7 31 32  0  0  0  0]\n",
            " [33 34 35 36  0  0  0]\n",
            " [37 38  4 39  0  0  0]\n",
            " [40 41  4 42  0  0  0]\n",
            " [43  8 44  2 45  0  0]\n",
            " [46  2 47 48 49  0  0]\n",
            " [ 5 50 51 52 53  0  0]\n",
            " [54 55  2 56  3 57  0]\n",
            " [ 6 58  2 59 60  3 61]]\n",
            "\n",
            "Shape de X_treino: (12, 7)\n",
            "shape de X_teste: (3, 7)\n",
            "Shape de y_treino: (12,)\n",
            "Shape de y_teste: (3,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Passo 3: Construção do Modelo LSTM ####"
      ],
      "metadata": {
        "id": "DOceMu7fp1qE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir a arquitetura do modelo LST™\n",
        "modelo_lstm = Sequential()\n",
        "\n",
        "# Camada de Embedding: Converte os índices numéricos das palavras em vetores densos.\n",
        "# total_palavras _vocab: tamanho do vocabulário\n",
        "# 50: dimensão do vetor de embedding (pode ser ajustado)\n",
        "# input_length: comprimento padronizado das sequências (max_len)\n",
        "modelo_lstm.add (Embedding(total_palavras_vocab, 50, input_length=max_len))\n",
        "\n",
        "# Camada LSTM:\n",
        "# 64: número de unidades (neurônios) na camada LSTM. Define o tamanho do estado oculto e da célula de memória.\n",
        "# dropou: Um tipo de regularização para evitar overfitting (descarta aleatoriamente neurônios durante o treinamento) .\n",
        "# recurrent_dropout: Dropout aplicado nas conexões recorrentes da LSTM.\n",
        "modelo_lstm.add (LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# Camada Densa de Saida:\n",
        "# 1: Um único neurônio de saída, pois é um problema de classificação binária (positivo/negativo).\n",
        "# activation='sigmoid': Função de ativação para classificação binária (produz um valor entre 0 e 1).\n",
        "modelo_lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compilar o modelo\n",
        "modelo_lstm. compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Exibir um resumo da arquitetura do modelo\n",
        "modelo_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "uBcLyjn3xDoS",
        "outputId": "64daecd7-e262-41ff-d3b7-421427dc961e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Passo 4: Treinamento e Avaliação do Modelo ####"
      ],
      "metadata": {
        "id": "8smiCsfzp--N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar o modelo\n",
        "print(\"\\nIniciando o treinamento do modelo LSTM...\")\n",
        "historico = modelo_lstm.fit(\n",
        "    X_treino, y_treino,\n",
        "    epochs=50,\n",
        "    batch_size=2,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Treinamento concluído!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLd1zUmXxUm3",
        "outputId": "af897675-0ba3-4e4b-fb96-89742cd00d80"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando o treinamento do modelo LSTM...\n",
            "Epoch 1/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 176ms/step - accuracy: 0.3972 - loss: 0.6959 - val_accuracy: 0.5000 - val_loss: 0.6910\n",
            "Epoch 2/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6528 - loss: 0.6871 - val_accuracy: 0.5000 - val_loss: 0.6904\n",
            "Epoch 3/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6653 - loss: 0.6849 - val_accuracy: 0.5000 - val_loss: 0.6909\n",
            "Epoch 4/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7931 - loss: 0.6839 - val_accuracy: 0.5000 - val_loss: 0.6917\n",
            "Epoch 5/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9667 - loss: 0.6751 - val_accuracy: 1.0000 - val_loss: 0.6922\n",
            "Epoch 6/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.6646 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
            "Epoch 7/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.6421 - val_accuracy: 0.5000 - val_loss: 0.6956\n",
            "Epoch 8/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.6187 - val_accuracy: 0.0000e+00 - val_loss: 0.7067\n",
            "Epoch 9/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.5587 - val_accuracy: 0.0000e+00 - val_loss: 0.7311\n",
            "Epoch 10/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.4509 - val_accuracy: 0.0000e+00 - val_loss: 0.8007\n",
            "Epoch 11/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.2852 - val_accuracy: 0.0000e+00 - val_loss: 0.9728\n",
            "Epoch 12/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1705 - val_accuracy: 0.0000e+00 - val_loss: 1.3468\n",
            "Epoch 13/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.1237 - val_accuracy: 0.0000e+00 - val_loss: 1.9377\n",
            "Epoch 14/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0423 - val_accuracy: 0.0000e+00 - val_loss: 2.5455\n",
            "Epoch 15/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0160 - val_accuracy: 0.0000e+00 - val_loss: 3.0169\n",
            "Epoch 16/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0091 - val_accuracy: 0.0000e+00 - val_loss: 3.3488\n",
            "Epoch 17/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0062 - val_accuracy: 0.0000e+00 - val_loss: 3.5878\n",
            "Epoch 18/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.0000e+00 - val_loss: 3.7812\n",
            "Epoch 19/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0039 - val_accuracy: 0.0000e+00 - val_loss: 3.9210\n",
            "Epoch 20/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.0000e+00 - val_loss: 4.0370\n",
            "Epoch 21/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.0000e+00 - val_loss: 4.1261\n",
            "Epoch 22/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.0000e+00 - val_loss: 4.2050\n",
            "Epoch 23/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.0000e+00 - val_loss: 4.2674\n",
            "Epoch 24/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.0000e+00 - val_loss: 4.3261\n",
            "Epoch 25/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.0000e+00 - val_loss: 4.3801\n",
            "Epoch 26/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.0000e+00 - val_loss: 4.4367\n",
            "Epoch 27/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.0000e+00 - val_loss: 4.4938\n",
            "Epoch 28/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.0000e+00 - val_loss: 4.5466\n",
            "Epoch 29/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.0000e+00 - val_loss: 4.5894\n",
            "Epoch 30/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.0000e+00 - val_loss: 4.6358\n",
            "Epoch 31/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.0000e+00 - val_loss: 4.6804\n",
            "Epoch 32/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.0000e+00 - val_loss: 4.7252\n",
            "Epoch 33/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.0000e+00 - val_loss: 4.7671\n",
            "Epoch 34/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.0000e+00 - val_loss: 4.8136\n",
            "Epoch 35/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.0000e+00 - val_loss: 4.8552\n",
            "Epoch 36/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.0000e+00 - val_loss: 4.8954\n",
            "Epoch 37/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 8.6354e-04 - val_accuracy: 0.0000e+00 - val_loss: 4.9405\n",
            "Epoch 38/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.0000e+00 - val_loss: 4.9812\n",
            "Epoch 39/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 7.5698e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.0216\n",
            "Epoch 40/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 9.1770e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.0629\n",
            "Epoch 41/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 6.8275e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.1046\n",
            "Epoch 42/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 7.5702e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.1417\n",
            "Epoch 43/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 6.8906e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.1746\n",
            "Epoch 44/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 6.5048e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.2080\n",
            "Epoch 45/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 8.1639e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.2449\n",
            "Epoch 46/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 6.2640e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.2822\n",
            "Epoch 47/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 6.4298e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.3142\n",
            "Epoch 48/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 7.3902e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.3463\n",
            "Epoch 49/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 7.7249e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.3830\n",
            "Epoch 50/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 5.3296e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.4153\n",
            "Treinamento concluído!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Avalia o modelo no conjunto de teste\n",
        "perda, acuracia = modelo_lstm.evaluate(X_teste, y_teste, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia do modelo no conjunto de teste: {acuracia*100:.2f}%\")\n",
        "print(f\"Perda do modelo no conjunto de teste: {perda: .4f}\")\n",
        "\n",
        "# Fazer previsões no conjunto de teste\n",
        "y_pred_prob = modelo_lstm.predict(X_teste)\n",
        "y_pred_classes = (y_pred_prob > 0.5).astype(int) # Converter probabilidades para 0 ou 1\n",
        "print(\"\\n--- Relatório de Classificação ---\")\n",
        "print(classification_report(y_teste, y_pred_classes, target_names=['negativo', 'positivo']))\n",
        "print(\"\\n--- Matriz de Confusão ---\")\n",
        "cm = confusion_matrix(y_teste, y_pred_classes)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[ 'negativo', 'positivo'], yticklabels=['negativo', 'positivo'])\n",
        "plt.xlabel('Previsto')\n",
        "plt.ylabel ('Real')\n",
        "plt.title('Matriz de Confusão')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "QTxZUbAFx-h0",
        "outputId": "028836de-6db5-4469-b184-bc4682af9692"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acurácia do modelo no conjunto de teste: 66.67%\n",
            "Perda do modelo no conjunto de teste:  0.7491\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432ms/step\n",
            "\n",
            "--- Relatório de Classificação ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo       0.50      1.00      0.67         1\n",
            "    positivo       1.00      0.50      0.67         2\n",
            "\n",
            "    accuracy                           0.67         3\n",
            "   macro avg       0.75      0.75      0.67         3\n",
            "weighted avg       0.83      0.67      0.67         3\n",
            "\n",
            "\n",
            "--- Matriz de Confusão ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Passo 5: Testar o Modelo com Novas Frases ####"
      ],
      "metadata": {
        "id": "bKr153zIqD1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utilizando o modelo treinado\n",
        "def prever_sentimento(modelo, tokenizer, max_seq_len, frase_nova, mapeamento_sentimento):\n",
        "  \"\"\"\"\n",
        "  Prevê o sentimento de uma nova frase.\n",
        "  \"\"\"\n",
        "  # Converter a frase para sequência numérica\n",
        "  sequencia_numerica = tokenizer.texts_to_sequences([frase_nova])\n",
        "\n",
        "  # Se a frase tem palavras desconhecidas, o tokenizer pode retornar uma lista vazia ou valores 0\n",
        "  if not sequencia_numerica:\n",
        "    print(f\"Aviso: A frase '{frase_nova}' contém apenas palavras desconhecidas.\")\n",
        "    return \"Desconhecido\" # Ou outra indicação\n",
        "\n",
        "  sequencia_numerica = sequencia_numerica[O] # Pega a primeira (e única) sequência\n",
        "\n",
        "  # Padronizar o comprimento da sequência de entrada\n",
        "  sequencia_padded = pad_sequences ([sequencia_numerica], maxlen=max_seq_len, padding= 'post')\n",
        "\n",
        "  # Fazer a previsão (probabilidade)\n",
        "  probabilidade_positiva = modelo.predict(sequencia_padded, verbose=0) [0][0]\n",
        "\n",
        "  # Inverter o mapeamento para obter o nome do sentimento\n",
        "  mapeamento_inverso = {v: k for k, v in mapeamento_sentimento.items()}\n",
        "\n",
        "  # Classificar com base no limiar de 0.5\n",
        "  if probabilidade_positiva >= 0.5:\n",
        "    return mapeamento_inverso[1] # 'positivo'\n",
        "  else:\n",
        "    return mapeamento_inverso[0] # 'negativo'\n",
        "\n",
        "# Testar o modelo com novas frases\n",
        "print(\"\\n--- Testando o Modelo LSTM com Novas Frases ---\")\n",
        "\n",
        "frase_nova_1 = \"gostei muito do filme, excelente!\"\n",
        "sentimento_1 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_1, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_1}' -> Sentimento previsto: '{sentimento_1}'\")\n",
        "\n",
        "frase_nova_2 = \"odiei o livro, muito entediante\"\n",
        "sentimento_2 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_2, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_2}' -> Sentimento previsto: '{sentimento_2}'\")\n",
        "\n",
        "frase_nova_3 = \"a aula de pin é ótima\"\n",
        "sentimento_3 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_3, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_3}' -> Sentimento previsto: '{sentimento_3}'\")\n",
        "\n",
        "frase_nova_4 = \"o atendimento foi péssimo\"\n",
        "sentimento_4 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_4, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_4}' -> Sentimento previsto: '{sentimento_4}'\")\n",
        "\n",
        "frase_nova_5 = \"esse produto não vale a pena, é caro\"\n",
        "sentimento_5 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_5, mapeamento_sentimento)\n",
        "print(f\"Frase: '(frase_nova_5)' -> Sentimento previsto: '{sentimento_5}'\")\n",
        "\n",
        "frase_nova_6 = \"o filme é legal\" # Frase curta e ambígua para um modelo pequeno\n",
        "sentimento_6 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_6, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_6}' -> Sentimento previsto: '{sentimento_6}'\")\n",
        "\n",
        "frase_nova_7 = \"isso é horrível, que tristeza\"\n",
        "sentimento_7 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_7, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_7}' -> Sentimento previsto: '{sentimento_7}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "7dpNXnDkqKaY",
        "outputId": "02175f6d-4b95-4ebc-85c7-c2d9a724d73c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testando o Modelo LSTM com Novas Frases ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'O' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-2292234785>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mfrase_nova_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gostei muito do filme, excelente!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0msentimento_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprever_sentimento\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelo_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrase_nova_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapeamento_sentimento\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Frase: '{frase_nova_1}' -> Sentimento previsto: '{sentimento_1}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-2292234785>\u001b[0m in \u001b[0;36mprever_sentimento\u001b[0;34m(modelo, tokenizer, max_seq_len, frase_nova, mapeamento_sentimento)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"Desconhecido\"\u001b[0m \u001b[0;31m# Ou outra indicação\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0msequencia_numerica\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequencia_numerica\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Pega a primeira (e única) sequência\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# Padronizar o comprimento da sequência de entrada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'O' is not defined"
          ]
        }
      ]
    }
  ]
}